<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-05-20T00:14:57+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Harin</title><subtitle>My little place in the internet</subtitle><author><name>Harin</name><email>iamharinramesh@gmail.com</email></author><entry><title type="html">Math behind Back-Propagation</title><link href="http://localhost:4000/2017-03-05/pagination-post" rel="alternate" type="text/html" title="Math behind Back-Propagation" /><published>2017-03-05T00:00:00+05:30</published><updated>2017-03-05T00:00:00+05:30</updated><id>http://localhost:4000/2017-03-05/pagination-post</id><content type="html" xml:base="http://localhost:4000/2017-03-05/pagination-post"><![CDATA[<p>Today let’s demystify the secret behind back-propagation. In this post, I will try to include all Math involved in back-propagation. Familiarity with basic calculus would be great. Otherwise, I don’t recommend continuing this article.</p>

<p>Let’s start with what is back-propagation? In simple terms, it computes the derivatives of the loss function with respect to weight and biases in a neural network. Algorithms such as gradient descent or stochastic gradient descent use gradient computed through back-propagation for optimization. That’s it for the intro, let’s dive into the Math.</p>

<p>Let‘s find the derivative of the softmax, this is something that we need later. We can apply Quotient rule to find derivative, since it is in the form g(x)/h(x).</p>

<p>There will be two cases, i equal to j and not equal to j.</p>

<p>Till now we haven’t computed any gradient w.r.t loss, we are building to it step by step.</p>

<p>Let’s consider the neural network shown blow.</p>

<p>z is the weighted sum of activation of the previous layer and corresponding weights and a is the activation of the neuron which is obtained by applying activation function to z. Now let’s look, how activation of a specific neuron is represented.</p>

<p>The above example represents the activation of 2nd neuron in 3 layer.</p>

<p>It is time to get into the meat and flesh.</p>

<p>Let’s find the derivative of loss w.r.t z in the output layer(l th layer). As I already mentioned, cross-entropy will be our loss.</p>

<p>k is the number of predictions, it will be equal to the number of neurons in the output layer.</p>

<p>We can use the derivative of softmax that we derived earlier. One thing to remember, we need to compute the derivative of softmax k times in which k-1 times i is not equal to k.</p>

<p>Let’s continue our derivation considering the above network.</p>

<p>Now let’s find the derivative of weights with respect to loss, we will be using chain rule here.</p>

<p>Bias units are not shown in the network, but they are present in the network.</p>]]></content><author><name>Harin</name></author><category term="NeuralNetworks" /><category term="DeepLearning" /><category term="MachineMearning" /><summary type="html"><![CDATA[Today let’s demystify the secret behind back-propagation. In this post, I will try to include all Math involved in back-propagation. Familiarity with basic calculus would be great. Otherwise, I don’t recommend continuing this article.]]></summary></entry></feed>