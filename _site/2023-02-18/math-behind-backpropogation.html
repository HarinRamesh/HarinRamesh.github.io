<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Math behind Back-Propagation | Harin</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Math behind Back-Propagation" />
<meta name="author" content="Harin" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My little place in the internet" />
<meta property="og:description" content="My little place in the internet" />
<link rel="canonical" href="http://localhost:4000/2023-02-18/math-behind-backpropogation" />
<meta property="og:url" content="http://localhost:4000/2023-02-18/math-behind-backpropogation" />
<meta property="og:site_name" content="Harin" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-02-18T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Math behind Back-Propagation" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Harin"},"dateModified":"2023-02-18T00:00:00+05:30","datePublished":"2023-02-18T00:00:00+05:30","description":"My little place in the internet","headline":"Math behind Back-Propagation","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023-02-18/math-behind-backpropogation"},"url":"http://localhost:4000/2023-02-18/math-behind-backpropogation"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">

  <!-- RSS -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Harin" />

  <!-- Google Analytics-->
  
</head>


  <body>

    <nav class="nav">
  <div class="nav-container">
    <a href="/">
      <h2 class="nav-title">Harin</h2>
    </a>
    <ul>
      <li><a href="/">Posts</a></li>
      <!-- <li><a href="/tags">Tags</a></li> -->
      <li><a href="/about">About</a></li>
    </ul>
  </div>
</nav>


    <main>
      <div class="post">

  <h3 class="post-title">Math behind Back-Propagation</h3>
  <div class="post-line"></div>

  <p><br /><br />
<img src="/assets/math-behind-back-propogation/image1.webp" alt="dass" />
<br /><br /></p>

<p>Today let’s demystify the secret behind back-propagation. In this post, I will try to include all Math involved in back-propagation. Familiarity with basic calculus would be great. Otherwise, I don’t recommend continuing this article.</p>

<p>Let’s start with what is back-propagation? In simple terms, it computes the derivatives of the loss function with respect to weight and biases in a neural network. Algorithms such as gradient descent or stochastic gradient descent use gradient computed through back-propagation for optimization. That’s it for the intro, let’s dive into the Math.
<br /><br />
<img src="/assets/math-behind-back-propogation/image2.webp" alt="dass" />
<br /><br /></p>

<p>Let‘s find the derivative of the softmax, this is something that we need later. We can apply Quotient rule to find derivative, since it is in the form g(x)/h(x).
<br /><br />
<img src="/assets/math-behind-back-propogation/sigmoid.webp" alt="" />
<br /><br />
There will be two cases, i equal to j and not equal to j.
<br /><br />
<img src="/assets/math-behind-back-propogation/1_zqQAluR8MC0G_7dTc8gFKw.webp" alt="" />
<br /><br />
Till now we haven’t computed any gradient w.r.t loss, we are building to it step by step.</p>

<p>Let’s consider the neural network shown blow.
<br /><br />
<img src="/assets/math-behind-back-propogation/1_gffcyukG88PzWqZcXgwLgg.webp" alt="" />
<br /><br /></p>

<p>z is the weighted sum of activation of the previous layer and corresponding weights and a is the activation of the neuron which is obtained by applying activation function to z. Now let’s look, how activation of a specific neuron is represented.
<br /><br />
<img src="/assets/math-behind-back-propogation/1_LxoLQZaDOfBbsfiqkbipJQ.webp" alt="" />
<br /><br />
The above example represents the activation of 2nd neuron in 3 layer.</p>

<p>It is time to get into the meat and flesh.</p>

<p>Let’s find the derivative of loss w.r.t z in the output layer(l th layer). As I already mentioned, cross-entropy will be our loss.
<br /><br />
<img src="/assets/math-behind-back-propogation/1_9puy_S8W5qVLCznLkkRvtg.webp" alt="" />
<br /><br />
k is the number of predictions, it will be equal to the number of neurons in the output layer.
<br /><br />
<img src="/assets/math-behind-back-propogation/1_rlIwOIEiBp4jBXTXIqV_nw.webp" alt="" />
<br /><br /></p>

<p>We can use the derivative of softmax that we derived earlier. One thing to remember, we need to compute the derivative of softmax k times in which k-1 times i is not equal to k.
<br /><br />
<img src="/assets/math-behind-back-propogation/1_hrIWjmCsWeau8GjcHsU7Qw.webp" alt="" />
<br /><br /></p>

<p>Let’s continue our derivation considering the above network.</p>

<p>Now let’s find the derivative of weights with respect to loss, we will be using chain rule here.
<br /><br />
<img src="/assets/math-behind-back-propogation/1_GXMG9LUD-HXLBGOamVb_4Q.webp" alt="" />
<br /><br /></p>

<p>Bias units are not shown in the network, but they are present in the network.
<br /><br />
<img src="/assets/math-behind-back-propogation/1_q8_qfYYjBxJWgV7oGPmEPQ.webp" alt="" />
<br /><br />
Derivative of activation in the previous layer are as follows.
<br /><br />
<img src="/assets/math-behind-back-propogation/1_tItTSn3gkWuBPIXilKA6sg.webp" alt="" />
<br /><br /></p>

<p>Here n is number of neurons in the current layer.</p>

<p>Let’s look at the derivatives of z in the hidden layer w.r.t loss.
<br /><br />
<img src="/assets/math-behind-back-propogation/1_fRqfWErG_vIY9uFN_u-tKQ.webp" alt="" />
<br /><br />
Here g’(z) is the derivative of the activation function used in the corresponding layer.</p>

<p>We have formed general equations to find the derivative of weights, bias and activation w.r.t loss.</p>

<p>The process of finding derivatives is as follows. We will first find the derivative of the last layer w.r.t loss then will use these derivative to find derivative of the second last layer, this process will continue until the input layer. This how back-propagation computes derivatives. Here the flow is from last to the first layer, which directly opposite to forward propagation.</p>

<p>Hope you enjoyed it. If you’ve made it this far and found any errors in any of the above or can think of any ways to make it clearer for future readers, don’t hesitate to drop a comment. Thanks!</p>

</div>



<div class="pagination">
  
  

  <a href="#" class="top">Top</a>
</div>
    </main>

    <footer>
  <span>
    <!-- &copy; <time datetime="2024-05-26 00:49:36 +0530">2024</time> Harin. -->
  </span>
</footer>

  </body>
</html>
